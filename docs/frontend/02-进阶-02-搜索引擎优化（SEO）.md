## 进阶篇：搜索引擎优化（SEO）

**搜索引擎优化（SEO，search engine optimization）**是一种用于提升网页在搜索引擎中的收录数量以及排序位置而做的优化行为。

简单一点来说，就是让搜索引擎能够更好的搜索到我们，让更多用户可以看到我们的网页。

### 搜索引擎工作原理

#### 什么是搜索引擎？

搜索引擎是根据用户需求与一定算法，运用特定策略从互联网检索出制定信息反馈给用户的一门检索技术。换一种说法，搜索引擎把计算机中存储的信息与用户的信息需求相匹配，并把匹配的结果展示出来。

搜索引擎依托于多种技术，如网络爬虫技术、检索排序技术、网页处理技术、大数据处理技术、自然语言处理技术等，为信息检索用户提供快速、高相关性的信息服务。搜索引擎技术的**核心模块一般包括爬虫、索引、检索和排序等**，同时可添加其他一系列辅助模块，以为用户创造更好的网络使用环境。

每个搜索引擎都包含两个主要部分：

* **搜索索引：** 有关网页信息的数字图书馆。
* **搜索算法：** 匹配搜索并进行排名的计算机程序。

现在流行的有百度、google、bing、搜狗等等搜索引擎。其中，百度是全国用的最多的，google 是全世界用的最多的。

[**搜索引擎大全 - 全球搜索引擎大全**](http://sousuo.org.cn/)

#### 什么是网络爬虫？

网络爬虫（又称网络蜘蛛、网络机器人）是一种从互联网抓取数据信息的自动化程序。

网络爬虫本质就是 http 请求。浏览器和网络爬虫是两种不同的网络客户端，但都以相同的方式来获取网页。不同的是：浏览器需要人工一步步去逐个得打开网页；而网络爬虫通过指定 url，自动将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。

网络爬虫的基本工作流程如下：

- 首先选取一部分精心挑选的种子 URL，放入待抓取 URL 队列；
- 从待抓取 URL 队列中取出待抓取在 URL，解析 DNS，并且得到主机的 IP，并将 URL 对应的网页下载下来，存储进已下载网页库中。此外，将这些 URL 放进已抓取URL队列。
- 分析已抓取 URL 队列中的 URL，分析其页面中的其他 URL，并且将 URL 放入待抓取 URL 队列，从而进入下一个循环。

**搜索引擎利用爬虫来创建索引是网络爬虫最典型的使用案例。**

网络爬虫是搜索引擎系统中十分重要的组成部分，它的优劣，很大程度上反映了一个搜索引擎的好差。网络爬虫负责从互联网中搜集网页，采集信息，这些网页信息用于建立索引从而为搜索引擎提供支持，它决定着整个引擎系统的内容是否丰富，信息是否及时，因此其性能的优劣直接影响着搜索引擎的效果。

#### 搜索引擎工作原理

搜索引擎为了以最快的速度得到搜索结果，它搜索的内容通常是预先整理好的网页索引数据库。它从索引中提取相关结果，并使用算法对其进行排名。

普通搜索，不能真正理解网页上的内容，它只能机械地匹配网页上的文字。真正意义上的搜索引擎，通常指的是收集了互联网上几千万到几十亿个网页并对网页中的每一个文字（即关键词）进行索引，建立索引数据库的全文搜索引擎。当用户查找某个关键词的时候，所有在页面内容中包含了该关键词的网页都将作为搜索结果被搜出来。在经过复杂的算法进行排序后，这些结果将按照与搜索关键词的相关度高低，依次排列。

典型的搜索引擎三大模块组成：**信息采集模块、查询表模块、检索模块。**

搜索引擎工作流程如下：

* **抓取网页（爬虫）**

  搜索引擎是通过一种特定规律的程序跟踪网页的链接，从一个链接爬到另外一个链接，像蜘蛛在蜘蛛网上爬行一样，所以被称为“蜘蛛”也被称为“机器人”。

  搜索引擎通过蜘蛛跟踪链接爬行到网页，并将爬行的数据存入原始页面数据库，以备建立索引库和用户检索。其中的页面数据与用户浏览器得到的 HTML 是完全一样的。搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。

* **预处理**

  搜索引擎将蜘蛛抓取回来的页面，进行各种步骤的预处理，包括：提取文字 -> 分词 -> 去停止词 -> 消除噪音（比如版权声明、导航条、广告等……） -> 正向索引 -> 倒排索引 -> 链接关系计算 -> 特殊文件处理。

  除了 HTML 文件外，搜索引擎通常还能抓取和索引以文字为基础的多种文件类型，如 PDF、Word、WPS、XLS、PPT、TXT 文件等。

  但搜索引擎还不能处理图片、视频、Flash 这类非文字内容，也不能执行脚本和程序。

* **对信息进行提取和组织建立索引库**

  首先是数据分析与标引，搜索引擎对已经收集到的资料给与按照网页中的字符特性予以分类，建立搜索原则。

  接下来是数据组织，搜索引擎负责形成规范的索引数据库或便于浏览的层次型分类目录结构，也就是计算网页等级。这个原则特别是在Google非常重要，一个接受很多链接的网页，搜索引擎必然在所有的网页当中将这些连接多的网页提升上来。

* **在索引数据库中搜索排序**

  由检索器根据用户输入的查询关键字，在索引库中快速检出文档，进行文档与查询的相关度评价，对将要输出的结果进行排序，并将查询结果返回给用户。

  搜索引擎还负责提取用户相关信息，利用这些信息来提高检索服务的质量，信息挖掘在个性化服务中起到关键作用。

#### 网络爬虫的发展趋势

随着 AJAX/Web2.0 的流行，如何抓取 AJAX 等动态页面成了搜索引擎急需解决的问题，如果搜索引擎依旧采用“爬”的机制，是无法抓取到AJAX 页面的有效数据的。 对于 AJAX 这样的技术，所需要的爬虫引擎必须是基于驱动的。而如果想要实现事件驱动，首先需要解决以下问题：

- JavaScript 的交互分析和解释；
- DOM 事件的处理和解释分发；
- 动态 DOM 内容语义的抽取。

### SEO 方法分类

SEO 方法可分为：**白帽 SEO、黑帽 SEO、灰帽SEO**。

* **白帽 SEO：** 使用符合主流搜索引擎发行方针规定的 SEO 优化方法。

  搜索引擎是以文本为中心，许多有助于网页亲和力的手段同样便利于搜索引擎优化。这些方法包括优化图形内容、ALT 属性、增加文本说明，甚至 Flash 动画可在设计该页时添加一些替代性内容。

* **黑帽 SEO：** 笼统的说，是指所有使用作弊手段或可疑手段的 SEO 优化方法。

  比如：垃圾链接、隐藏网页、刷 IP 流量、桥页、关键词堆砌等等。

  黑帽 SEO 的主要目的，是让网站得到他们所希望的排名进而获得更多的曝光率，这可能导致令普通用户不满的搜索结果。因此搜索引擎一旦发现使用“黑帽”技术的网站，轻则降低其排名，重则从搜索结果中永远剔除该网站。

* **灰帽SEO：** 是指介于白帽与黑帽之间的中间地带。对于白帽而言，会采取一些取巧的手法，这些行为因为不算违规，但同样也不遵守规则，是为灰色地带。

  它注重了优化的整体与局部的方方面面，追求的是某种程度的中庸，既考虑长期利益，也要考虑短期收益问题。

从SEO 的优化方向可分为：**技术、撰稿、声望**。

* **技术：** 使用语义标记内容HTML。浏览网站时，抓取工具应该只找到您要编入索引的内容。
* **撰稿：** 使用访问者的词汇编写内容。 使用文本和图像，以便抓取工具可以理解主题。
* **声望：** 当其他已建立的站点链接到您的站点时，您获得最多流量。

### 如何做好 SEO？

SEO 最主要的目的是**让网站能获得更多点击和自然流量**，其中最重要的一种手段是**提升网站在搜索引擎的自然搜索结果的排名**。

每个搜索引擎都有用于对网页进行排名的独特算法，大型搜索引擎都将结果排名保持为商业秘密。 没有人知道这些排名具体是如何计算出来的，但是，一些关键因素却是已知的。

#### 搜索引擎如何对网页进行排名？

Google 的搜索质量高级策略师 `Andrey Lipattsev` 在2016年的一次在线网络研讨会上，被问到两个最重要的排名因素时，他的回答很简单：**内容和链接。**

Google 有 200 多个排名因素。没有人知道所有的这些排名因素，但是其中的关键因素包含：

- 外链： 链接到页面的网站数量和质量。
- 相关性：查找包含与搜索词相同的关键词的页面， 另外还使用交互数据来评估搜索结果是否与搜索词相关。
- 新鲜度
- 话题权威性
- 页面速度
- 移动友好

#### SEO 的四个主要方面

- **关键词研究：**寻找客户正在搜索的内容、这些词可以为你带来多少流量、以及为他们进行排名的难度。
- **页面 SEO：**一种优化网页以在搜索引擎中排名更高的做法。 它包括对可见内容和 HTML 源代码的优化。
- **链接建设：**友情链接，好的友情链接可以快速的提高你的网站权重；外链，高质量的外链，会给你的网站提高源源不断的权重提升。
- **技术性 SEO：**一个优化网站以帮助诸如 Google 之类的搜索引擎查找、理解和索引该页面的过程。

### 如何做好页面 SEO？

页面 SEO（也称为站点 SEO）是一种优化网页以在搜索引擎中排名更高的做法。 它包括对可见内容和 HTML 源代码的优化。

#### 内容优化

页面内容除了需要一个主要的目标关键词，还需要注意以下四点：

- 具有相关性：相关性可以说是页面搜索引擎优化中最关键的部分，这意味着将你的内容与搜索者的搜索意图结合起来。
- 具有全面性：页面内容要覆盖搜索者期待和想要看到的所有东西。
- 具有唯一性：页面内容需要带来一些新的东西，以区别与其他页面。
- 具有清晰度：页面内容是人们想要阅读的清晰内容。

#### HTML源代码优化

* **精选摘要：**合理的设计 `title`、`description` 和 `keywords`。这三个的权重逐个减小。

  * **title：** 强调重点即可，重要关键词出现不要超过 2 次。尽量把重要的关键词放在前面且不要重复出现，尽量做到每个页面的标题中不要设置相同的内容。
  * **description：** 页面内容高度概括，长度合适，不可过分堆砌关键词，不同页面描述要有所不同。
  * **keyword：** 关键词，列举出几个页面的重要关键字即可，切记过分堆砌。

  ```html
  <title>前端搜索引擎优化的技巧</title>
  <meta name='description' content='介绍搜索引擎优化的技巧，如使用创建title标题、meta关键词和描述、语义化标签、img的alt属性等。'>
  <meta name='keywords' content='SEO,title,meta,语义化,alt'>
  ```

* **语义化书写 HTML 源码，符合 W3C 标准：** 语义化代码让阅读源码者和搜索引擎容易理解网页。

  * **语义化标签：** 在适当的位置使用适当的标签，用正确的标签做正确的事。

    根据内容的结构化，选择合适的 HTML5 标签尽量让代码语义化，如使用 `header、footer、section、aside、article、nav` 等等语义化标签可以让爬虫更好的解析。

  * **正确使用 a 标签**

    页内链接要加 `title` 属性加以说明，让访客和爬虫知道；而外部链接则需要加上 `rel="nofollow"` 属性, 告诉爬虫不要爬，因为一旦爬虫爬了外部链接之后，就不会再回来了。

    ```html
    <!--用于 meta 元标签，告诉爬虫该页面上所有链接都无需追踪。-->
    <meta name="robots" content="nofollow" />
    <!--用于 a 标签，告诉爬虫该页面无需追踪。-->
    <a href="https://www.xxxx?login" rel="nofollow">登录/注册</a>
    ```

    避免使用 `click` 事件来操作页面跳转，因为网络爬虫无法识别 Js 中的链接地址。

  * **合理使用 h1~h6 标签：**h1 标签自带权重，一般认为是仅次于标题的，一个页面有且最多只能有一个 H1 标签，用于页面最重要的标题；h2 标签通常作为二级标题或文章的小标题，可以重复使用；其余 h3~h6 标签如要使用应按顺序层层嵌套下去，不可以断层或反序。

  * **正确使用标签的 alt 和 title 属性：** 非装饰性 `img` 标签必须加 alt 属性，使网络爬虫可以抓取到图片的信息；`a` 标签加 title 属性，其实就是提示文字作用，当鼠标移动到该超链接上时，就会有提示文字的出现，title 属性也有利于网络爬虫抓取信息；其他需要强调的标签，也可以加上 title 属性。

  * **br标签：** 只用于文本内容的换行。

  * **表格标题：** 使用 `caption` 标签。

  * **strong、em标签：** strong 标签在搜索引擎中能够得到高度的重视，它能突出关键词，表现重要的内容；em 标签强调效果仅次于 strong 标签。避免使用 b、i 标签，对搜索引擎不友好。

  * **少用特殊符号：** 文本缩进不要使用特殊符号 `&nbsp`，应当使用 CSS 进行设置；版权符号不要使用特殊符号 `&copy`，直接使用版权符号©。

  * 谨慎使用 `display：none`：网络爬虫会过滤掉 `display:none` 的内容。

  * **404 页面：** 404 页面首先是用户体验良好，不会莫名报一些其他提示。其次对网络爬虫也友好，不会因为页面错误而停止抓取，可以返回抓取网站其他页面。

* **尽量保证 HTML 的纯粹和高质量：** 尽量让结构（HTML）、表现（CSS）及行为（JavaScript）三者分离； HTML 文档内容比较独特丰富（合理插入图片说明）等，会被认为质量较高符合用户需求，从而提高 SEO 的排名。

* **重要内容优先加载：**重要内容 HTML 代码放在最前。搜索引擎抓取 HTML 顺序是从上到下，有的搜索引擎对抓取长度有限制。

* 重要内容不要用 `js\iframe\flash\img\video` 形式：网络爬虫不会去抓取这些内容。

### 如何做好技术 SEO？

技术 SEO 是一个优化网站以帮助诸如 Google 之类的搜索引擎查找、理解和索引该页面的过程。

- **确保重要内容“可爬网”和“可索引”**

  爬网是搜索引擎发现最新内容的方式。蜘蛛在这里访问并从已知网页下载新数据。

  `robots.txt` 是一个告诉搜索引擎可以爬网和不能爬网哪些页面的文件。

  然而，可爬网页面并不总是可索引的。如果你的网页存在 `meta robots` 标签或 `x‑robots` 标题设置为 `noindex`，则搜索引擎无法索引该页面。

  ```html
  <meta name="robots" content="noindex" />
  ```

  ```shell
  Header set X-Robots-Tag “noindex”
  ```

- **建立网站地图：** 网站地图一个将网站栏目和连接归类的一个文件，让搜索引擎全面收录站点网页地址，了解站点网页地址的权重分布以及站点内容更新情况，提高爬虫的爬取效率。

  一位 Google 代表在 2019 年确认了网站地图的重要性，指出网站地图是 Google 第二重要的 URL 来源。

  原因之一是网站地图通常包含“孤立”页面，这些是搜索引擎无法通过爬网找到的页面，因为它们没有来自你的网站上可爬网页面的内部链接。

- **使用HTTPS（HTTP 可以重定向到 HTTPS）**

  HTTPS 加密网站和访问者之间发送的数据。它有助于保护敏感信息（如信用卡的详细信息）不被泄露。自2014年以来，它一直是排名因素。

- **扁平化网站结构：**一般而言，建立的网站结构层次越少，越容易被网络爬虫抓取，也就容易被收录。一般中小型网站目录结构超过三级，网络爬虫便不愿意往下爬了。

- **提高网站速度**： 网站速度是搜索引擎排序的一个重要指标。当网站速度很慢时，一旦超时，爬虫也会离开。
- 不要对内部链接运用 nofollow： `rel="nofollow"`链接通常用于标记指向你不想认可的页面的出站链接。

- 使用 hreflang 获得多语言内容：hreflang 是一个 HTML 属性，用于指定网页的语言和地理目标。它可用于其他语言版本的网页或其他地理位置定位的网站。

  ```html
  <link rel="alternate" hreflang="de" href="https://ahrefs.com/blog/de/kostenlose-keyword-recherche-tools/" />
  ```

- 去除重复内容

- 修复孤立页面：孤立页面是网站没有链接链入的内部链接。



### 参考资料

[ahrefs SEO新手指南](https://ahrefs.com/zh/seo)

[网络爬虫教程](https://piaosanlang.gitbooks.io/spiders/content/01day/README1.html)